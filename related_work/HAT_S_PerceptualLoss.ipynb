{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNpdZQwzMK2w"
      },
      "source": [
        "# Clone git repo and install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoU-bDEWLuWm",
        "outputId": "2e551be0-db40-434f-c887-1beee4467ed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HAT'...\n",
            "remote: Enumerating objects: 401, done.\u001b[K\n",
            "remote: Counting objects: 100% (242/242), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 401 (delta 192), reused 159 (delta 125), pack-reused 159\u001b[K\n",
            "Receiving objects: 100% (401/401), 20.72 MiB | 35.77 MiB/s, done.\n",
            "Resolving deltas: 100% (222/222), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/XPixelGroup/HAT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDOgxnzCMYik",
        "outputId": "7f1c1e76-72c3-457b-a889-257ea78c43d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HAT\n"
          ]
        }
      ],
      "source": [
        "%cd HAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihWBOSxbMbwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712586e7-b604-436b-80da-bd9734f8e00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops (from -r requirements.txt (line 1))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m936.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.1+cu118)\n",
            "Collecting basicsr==1.3.4.9 (from -r requirements.txt (line 3))\n",
            "  Downloading basicsr-1.3.4.9.tar.gz (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.2/161.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from basicsr==1.3.4.9->-r requirements.txt (line 3))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.18.3)\n",
            "Collecting lmdb (from basicsr==1.3.4.9->-r requirements.txt (line 3))\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.11.3)\n",
            "Collecting tb-nightly (from basicsr==1.3.4.9->-r requirements.txt (line 3))\n",
            "  Downloading tb_nightly-2.15.0a20231012-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (4.66.1)\n",
            "Collecting yapf (from basicsr==1.3.4.9->-r requirements.txt (line 3))\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->-r requirements.txt (line 2)) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->-r requirements.txt (line 2)) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->-r requirements.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2023.7.22)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.31.5)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.5)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->basicsr==1.3.4.9->-r requirements.txt (line 3)) (6.8.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.11.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.2.2)\n",
            "Building wheels for collected packages: basicsr\n",
            "  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for basicsr: filename=basicsr-1.3.4.9-py3-none-any.whl size=194421 sha256=2985690895a32e5a35ea35f9209744e93a386e7021f6bec23227907ef884b33a\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/19/97/8206d928857f2ef0625cb621b689005610515b4841be4e5ddb\n",
            "Successfully built basicsr\n",
            "Installing collected packages: lmdb, addict, einops, yapf, tb-nightly, basicsr\n",
            "Successfully installed addict-2.4.0 basicsr-1.3.4.9 einops-0.7.0 lmdb-1.4.1 tb-nightly-2.15.0a20231012 yapf-0.40.2\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py:84: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Requirements should be satisfied by a PEP 517 installer.\n",
            "        If you are using pip, you can try `pip install --use-pep517`.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  dist.fetch_build_eggs(dist.setup_requires)\n",
            "running develop\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "creating hat.egg-info\n",
            "writing hat.egg-info/PKG-INFO\n",
            "writing dependency_links to hat.egg-info/dependency_links.txt\n",
            "writing requirements to hat.egg-info/requires.txt\n",
            "writing top-level names to hat.egg-info/top_level.txt\n",
            "writing manifest file 'hat.egg-info/SOURCES.txt'\n",
            "reading manifest file 'hat.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'hat.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.10/dist-packages/hat.egg-link (link to .)\n",
            "Adding hat 0.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/HAT\n",
            "Processing dependencies for hat==0.1.0\n",
            "Searching for basicsr==1.3.4.9\n",
            "Best match: basicsr 1.3.4.9\n",
            "Adding basicsr 1.3.4.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for torch==2.0.1+cu118\n",
            "Best match: torch 2.0.1+cu118\n",
            "Adding torch 2.0.1+cu118 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for einops==0.7.0\n",
            "Best match: einops 0.7.0\n",
            "Adding einops 0.7.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for yapf==0.40.2\n",
            "Best match: yapf 0.40.2\n",
            "Adding yapf 0.40.2 to easy-install.pth file\n",
            "Installing yapf script to /usr/local/bin\n",
            "Installing yapf-diff script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tqdm==4.66.1\n",
            "Best match: tqdm 4.66.1\n",
            "Adding tqdm 4.66.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for torchvision==0.15.2+cu118\n",
            "Best match: torchvision 0.15.2+cu118\n",
            "Adding torchvision 0.15.2+cu118 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tb-nightly==2.15.0a20231012\n",
            "Best match: tb-nightly 2.15.0a20231012\n",
            "Adding tb-nightly 2.15.0a20231012 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for scipy==1.11.3\n",
            "Best match: scipy 1.11.3\n",
            "Adding scipy 1.11.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for scikit-image==0.19.3\n",
            "Best match: scikit-image 0.19.3\n",
            "Adding scikit-image 0.19.3 to easy-install.pth file\n",
            "Installing skivi script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for requests==2.31.0\n",
            "Best match: requests 2.31.0\n",
            "Adding requests 2.31.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for PyYAML==6.0.1\n",
            "Best match: PyYAML 6.0.1\n",
            "Adding PyYAML 6.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for Pillow==9.4.0\n",
            "Best match: Pillow 9.4.0\n",
            "Adding Pillow 9.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for opencv-python==4.8.0.76\n",
            "Best match: opencv-python 4.8.0.76\n",
            "Adding opencv-python 4.8.0.76 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for numpy==1.23.5\n",
            "Best match: numpy 1.23.5\n",
            "Adding numpy 1.23.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.10 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for lmdb==1.4.1\n",
            "Best match: lmdb 1.4.1\n",
            "Adding lmdb 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for future==0.18.3\n",
            "Best match: future 0.18.3\n",
            "Adding future 0.18.3 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for addict==2.4.0\n",
            "Best match: addict 2.4.0\n",
            "Adding addict 2.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for triton==2.0.0\n",
            "Best match: triton 2.0.0\n",
            "Adding triton 2.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for Jinja2==3.1.2\n",
            "Best match: Jinja2 3.1.2\n",
            "Adding Jinja2 3.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for networkx==3.1\n",
            "Best match: networkx 3.1\n",
            "Adding networkx 3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for sympy==1.12\n",
            "Best match: sympy 1.12\n",
            "Adding sympy 1.12 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for typing-extensions==4.5.0\n",
            "Best match: typing-extensions 4.5.0\n",
            "Adding typing-extensions 4.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for filelock==3.12.4\n",
            "Best match: filelock 3.12.4\n",
            "Adding filelock 3.12.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tomli==2.0.1\n",
            "Best match: tomli 2.0.1\n",
            "Adding tomli 2.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for platformdirs==3.11.0\n",
            "Best match: platformdirs 3.11.0\n",
            "Adding platformdirs 3.11.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for importlib-metadata==6.8.0\n",
            "Best match: importlib-metadata 6.8.0\n",
            "Adding importlib-metadata 6.8.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for werkzeug==3.0.0\n",
            "Best match: werkzeug 3.0.0\n",
            "Adding werkzeug 3.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tensorboard-data-server==0.7.1\n",
            "Best match: tensorboard-data-server 0.7.1\n",
            "Adding tensorboard-data-server 0.7.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for six==1.16.0\n",
            "Best match: six 1.16.0\n",
            "Adding six 1.16.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for setuptools==67.7.2\n",
            "Best match: setuptools 67.7.2\n",
            "Adding setuptools 67.7.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for protobuf==3.20.3\n",
            "Best match: protobuf 3.20.3\n",
            "Adding protobuf 3.20.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for Markdown==3.5\n",
            "Best match: Markdown 3.5\n",
            "Adding Markdown 3.5 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for google-auth-oauthlib==1.0.0\n",
            "Best match: google-auth-oauthlib 1.0.0\n",
            "Adding google-auth-oauthlib 1.0.0 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for google-auth==2.17.3\n",
            "Best match: google-auth 2.17.3\n",
            "Adding google-auth 2.17.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for grpcio==1.59.0\n",
            "Best match: grpcio 1.59.0\n",
            "Adding grpcio 1.59.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for absl-py==1.4.0\n",
            "Best match: absl-py 1.4.0\n",
            "Adding absl-py 1.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for packaging==23.2\n",
            "Best match: packaging 23.2\n",
            "Adding packaging 23.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for PyWavelets==1.4.1\n",
            "Best match: PyWavelets 1.4.1\n",
            "Adding PyWavelets 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tifffile==2023.9.26\n",
            "Best match: tifffile 2023.9.26\n",
            "Adding tifffile 2023.9.26 to easy-install.pth file\n",
            "Installing lsm2bin script to /usr/local/bin\n",
            "Installing tiff2fsspec script to /usr/local/bin\n",
            "Installing tiffcomment script to /usr/local/bin\n",
            "Installing tifffile script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for imageio==2.31.5\n",
            "Best match: imageio 2.31.5\n",
            "Adding imageio 2.31.5 to easy-install.pth file\n",
            "Installing imageio_download_bin script to /usr/local/bin\n",
            "Installing imageio_remove_bin script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for certifi==2023.7.22\n",
            "Best match: certifi 2023.7.22\n",
            "Adding certifi 2023.7.22 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for urllib3==2.0.6\n",
            "Best match: urllib3 2.0.6\n",
            "Adding urllib3 2.0.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for idna==3.4\n",
            "Best match: idna 3.4\n",
            "Adding idna 3.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for charset-normalizer==3.3.0\n",
            "Best match: charset-normalizer 3.3.0\n",
            "Adding charset-normalizer 3.3.0 to easy-install.pth file\n",
            "Installing normalizer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for lit==17.0.2\n",
            "Best match: lit 17.0.2\n",
            "Adding lit 17.0.2 to easy-install.pth file\n",
            "Installing lit script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for cmake==3.27.6\n",
            "Best match: cmake 3.27.6\n",
            "Adding cmake 3.27.6 to easy-install.pth file\n",
            "Installing cmake script to /usr/local/bin\n",
            "Installing cpack script to /usr/local/bin\n",
            "Installing ctest script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for MarkupSafe==2.1.3\n",
            "Best match: MarkupSafe 2.1.3\n",
            "Adding MarkupSafe 2.1.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for zipp==3.17.0\n",
            "Best match: zipp 3.17.0\n",
            "Adding zipp 3.17.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for requests-oauthlib==1.3.1\n",
            "Best match: requests-oauthlib 1.3.1\n",
            "Adding requests-oauthlib 1.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for rsa==4.9\n",
            "Best match: rsa 4.9\n",
            "Adding rsa 4.9 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for pyasn1-modules==0.3.0\n",
            "Best match: pyasn1-modules 0.3.0\n",
            "Adding pyasn1-modules 0.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for cachetools==5.3.1\n",
            "Best match: cachetools 5.3.1\n",
            "Adding cachetools 5.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for oauthlib==3.2.2\n",
            "Best match: oauthlib 3.2.2\n",
            "Adding oauthlib 3.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for pyasn1==0.5.0\n",
            "Best match: pyasn1 0.5.0\n",
            "Adding pyasn1 0.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for hat==0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "!python setup.py develop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k8fghMPT3BB",
        "outputId": "a55d32b2-5cc8-48fc-b38b-373b53e9b802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HAT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIHZT7muMqCn"
      },
      "source": [
        "# Process data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount drive"
      ],
      "metadata": {
        "id": "QwpFCViEU7eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H40FqfVQtHs",
        "outputId": "ca23cb6e-7744-490f-918e-c309f785e93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process train dataset"
      ],
      "metadata": {
        "id": "UukyW979U-44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Datasets/train/DIV2K_train_HR.zip -d /content/HAT/datasets/DIV2K\n",
        "!unzip /content/drive/MyDrive/Datasets/train/DIV2K_train_LR_bicubic_X4.zip -d /content/HAT/datasets/DIV2K"
      ],
      "metadata": {
        "id": "Nk_Gh2X6VctU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# naming train dataset\n",
        "import os\n",
        "for filename in os.listdir(\"/content/HAT/datasets/DIV2K/DIV2K_train_LR_bicubic/X4\"):\n",
        "    old_file_path = os.path.join(\"/content/HAT/datasets/DIV2K/DIV2K_train_LR_bicubic/X4\", filename)\n",
        "    new_filename = filename[0:4] + filename[6:]\n",
        "    new_file_path = os.path.join(\"/content/HAT/datasets/DIV2K/DIV2K_train_LR_bicubic/X4\", new_filename)\n",
        "    os.rename(old_file_path, new_file_path)"
      ],
      "metadata": {
        "id": "E3HcITKIht-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip validation dataset"
      ],
      "metadata": {
        "id": "ri1aj57yVIIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Datasets/test/Set5.zip -d /content/HAT/datasets\n",
        "!unzip /content/drive/MyDrive/Datasets/test/Set14.zip -d /content/HAT/datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHIpa0HPQtJP",
        "outputId": "c035c862-6ffb-4392-c0cc-2f897a5c728e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Datasets/test/Set5.zip\n",
            "   creating: /content/HAT/datasets/Set5/\n",
            "   creating: /content/HAT/datasets/Set5/GTmod12/\n",
            "  inflating: /content/HAT/datasets/Set5/GTmod12/baby.png  \n",
            "  inflating: /content/HAT/datasets/Set5/GTmod12/bird.png  \n",
            "  inflating: /content/HAT/datasets/Set5/GTmod12/butterfly.png  \n",
            "  inflating: /content/HAT/datasets/Set5/GTmod12/head.png  \n",
            "  inflating: /content/HAT/datasets/Set5/GTmod12/woman.png  \n",
            "   creating: /content/HAT/datasets/Set5/LRbicx2/\n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx2/baby.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx2/bird.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx2/butterfly.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx2/head.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx2/woman.png  \n",
            "   creating: /content/HAT/datasets/Set5/LRbicx3/\n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx3/baby.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx3/bird.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx3/butterfly.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx3/head.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx3/woman.png  \n",
            "   creating: /content/HAT/datasets/Set5/LRbicx4/\n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx4/baby.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx4/bird.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx4/butterfly.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx4/head.png  \n",
            "  inflating: /content/HAT/datasets/Set5/LRbicx4/woman.png  \n",
            "   creating: /content/HAT/datasets/Set5/original/\n",
            "  inflating: /content/HAT/datasets/Set5/original/baby.png  \n",
            "  inflating: /content/HAT/datasets/Set5/original/bird.png  \n",
            "  inflating: /content/HAT/datasets/Set5/original/butterfly.png  \n",
            "  inflating: /content/HAT/datasets/Set5/original/head.png  \n",
            "  inflating: /content/HAT/datasets/Set5/original/woman.png  \n",
            "Archive:  /content/drive/MyDrive/Datasets/test/Set14.zip\n",
            "   creating: /content/HAT/datasets/Set14/\n",
            "   creating: /content/HAT/datasets/Set14/GTmod12/\n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/baboon.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/barbara.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/bridge.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/coastguard.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/comic.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/face.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/flowers.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/foreman.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/lenna.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/man.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/monarch.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/pepper.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/ppt3.png  \n",
            "  inflating: /content/HAT/datasets/Set14/GTmod12/zebra.png  \n",
            "   creating: /content/HAT/datasets/Set14/LRbicx2/\n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/baboon.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/barbara.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/bridge.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/coastguard.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/comic.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/face.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/flowers.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/foreman.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/lenna.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/man.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/monarch.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/pepper.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/ppt3.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx2/zebra.png  \n",
            "   creating: /content/HAT/datasets/Set14/LRbicx3/\n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/baboon.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/barbara.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/bridge.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/coastguard.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/comic.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/face.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/flowers.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/foreman.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/lenna.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/man.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/monarch.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/pepper.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/ppt3.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx3/zebra.png  \n",
            "   creating: /content/HAT/datasets/Set14/LRbicx4/\n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/baboon.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/barbara.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/bridge.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/coastguard.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/comic.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/face.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/flowers.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/foreman.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/lenna.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/man.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/monarch.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/pepper.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/ppt3.png  \n",
            "  inflating: /content/HAT/datasets/Set14/LRbicx4/zebra.png  \n",
            "   creating: /content/HAT/datasets/Set14/original/\n",
            "  inflating: /content/HAT/datasets/Set14/original/baboon.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/barbara.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/bridge.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/coastguard.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/comic.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/face.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/flowers.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/foreman.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/lenna.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/man.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/monarch.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/pepper.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/ppt3.png  \n",
            "  inflating: /content/HAT/datasets/Set14/original/zebra.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVBxrG-4MqEm"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/HAT/experiments/train_HAT-S_SRx4_from_scratch/\n",
        "!mkdir /content/HAT/experiments/train_HAT-S_SRx4_from_scratch/models/\n",
        "!mkdir /content/HAT/experiments/train_HAT-S_SRx4_from_scratch/training_states/"
      ],
      "metadata": {
        "id": "vIzg7JM4_d2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy checkpoint to the required folder"
      ],
      "metadata": {
        "id": "ApbwbGcRDZi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/Results/superresolution/HAT-S_SRx4_from_scratch/models/net_g_3000.pth /content/HAT/experiments/train_HAT-S_SRx4_from_scratch/models/net_g_3000.pth\n",
        "!cp -r /content/drive/MyDrive/Results/superresolution/HAT-S_SRx4_from_scratch/training_states/3000.state /content/HAT/experiments/train_HAT-S_SRx4_from_scratch/training_states/3000.state"
      ],
      "metadata": {
        "id": "-BJlPM9DCitK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload train_HAT-S_SRx4_PerceptualLoss.yml into /content/HAT/options/train directory and modify  it"
      ],
      "metadata": {
        "id": "a4s60CHicUm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwIeuYR6Mpmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54495a57-7ae1-4f9f-f70c-a4ab831e5508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "Disable distributed.\n",
            "Path already exists. Rename it to /content/HAT/experiments/train_HAT-S_SRx4_PerceptualLoss_archived_20231012_183540\n",
            "Path already exists. Rename it to /content/HAT/tb_logger/train_HAT-S_SRx4_PerceptualLoss_archived_20231012_183540\n",
            "2023-10-12 18:35:40,690 INFO: \n",
            "                ____                _       _____  ____\n",
            "               / __ ) ____ _ _____ (_)_____/ ___/ / __ \\\n",
            "              / __  |/ __ `// ___// // ___/\\__ \\ / /_/ /\n",
            "             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/\n",
            "            /_____/ \\__,_//____//_/ \\___//____//_/ |_|\n",
            "     ______                   __   __                 __      __\n",
            "    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /\n",
            "   / / __ / __ \\ / __ \\ / __  /  / /   / / / // ___// //_/  / /\n",
            "  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/\n",
            "  \\____/ \\____/ \\____/ \\____/  /_____/\\____/ \\___//_/|_|  (_)\n",
            "    \n",
            "Version Information: \n",
            "\tBasicSR: 1.3.4.9\n",
            "\tPyTorch: 2.0.1+cu118\n",
            "\tTorchVision: 0.15.2+cu118\n",
            "2023-10-12 18:35:40,690 INFO: \n",
            "  name: train_HAT-S_SRx4_PerceptualLoss\n",
            "  model_type: HATModel\n",
            "  scale: 4\n",
            "  num_gpu: 1\n",
            "  manual_seed: 0\n",
            "  datasets:[\n",
            "    train:[\n",
            "      name: DIV2K\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: datasets/DIV2K/DIV2K_train_HR\n",
            "      dataroot_lq: datasets/DIV2K/DIV2K_train_LR_bicubic/X4\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      gt_size: 256\n",
            "      use_hflip: True\n",
            "      use_rot: True\n",
            "      use_shuffle: True\n",
            "      num_worker_per_gpu: 2\n",
            "      batch_size_per_gpu: 4\n",
            "      dataset_enlarge_ratio: 1\n",
            "      prefetch_mode: None\n",
            "      phase: train\n",
            "      scale: 4\n",
            "    ]\n",
            "    val:[\n",
            "      name: Set5\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: datasets/Set5/GTmod12\n",
            "      dataroot_lq: datasets/Set5/LRbicx4\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      phase: val\n",
            "      scale: 4\n",
            "    ]\n",
            "    val_2:[\n",
            "      name: Set14\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: datasets/Set14/GTmod12\n",
            "      dataroot_lq: datasets/Set14/LRbicx4\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      phase: val\n",
            "      scale: 4\n",
            "    ]\n",
            "  ]\n",
            "  network_g:[\n",
            "    type: HAT\n",
            "    upscale: 4\n",
            "    in_chans: 3\n",
            "    img_size: 64\n",
            "    window_size: 16\n",
            "    compress_ratio: 24\n",
            "    squeeze_factor: 24\n",
            "    conv_scale: 0.01\n",
            "    overlap_ratio: 0.5\n",
            "    img_range: 1.0\n",
            "    depths: [6, 6, 6, 6, 6, 6]\n",
            "    embed_dim: 144\n",
            "    num_heads: [6, 6, 6, 6, 6, 6]\n",
            "    mlp_ratio: 2\n",
            "    upsampler: pixelshuffle\n",
            "    resi_connection: 1conv\n",
            "  ]\n",
            "  path:[\n",
            "    pretrain_network_g: None\n",
            "    strict_load_g: False\n",
            "    resume_state: None\n",
            "    experiments_root: /content/HAT/experiments/train_HAT-S_SRx4_PerceptualLoss\n",
            "    models: /content/HAT/experiments/train_HAT-S_SRx4_PerceptualLoss/models\n",
            "    training_states: /content/HAT/experiments/train_HAT-S_SRx4_PerceptualLoss/training_states\n",
            "    log: /content/HAT/experiments/train_HAT-S_SRx4_PerceptualLoss\n",
            "    visualization: /content/HAT/experiments/train_HAT-S_SRx4_PerceptualLoss/visualization\n",
            "  ]\n",
            "  train:[\n",
            "    ema_decay: 0.999\n",
            "    optim_g:[\n",
            "      type: Adam\n",
            "      lr: 0.0001\n",
            "      weight_decay: 0\n",
            "      betas: [0.9, 0.99]\n",
            "    ]\n",
            "    scheduler:[\n",
            "      type: MultiStepLR\n",
            "      milestones: [125000, 200000, 225000, 240000]\n",
            "      gamma: 0.5\n",
            "    ]\n",
            "    total_iter: 250000\n",
            "    warmup_iter: -1\n",
            "    perceptual_opt:[\n",
            "      type: PerceptualLoss\n",
            "      layer_weights:[\n",
            "        conv5_4: 1.0\n",
            "      ]\n",
            "    ]\n",
            "  ]\n",
            "  val:[\n",
            "    val_freq: 5000.0\n",
            "    save_img: True\n",
            "    pbar: False\n",
            "    metrics:[\n",
            "      psnr:[\n",
            "        type: calculate_psnr\n",
            "        crop_border: 4\n",
            "        test_y_channel: True\n",
            "        better: higher\n",
            "      ]\n",
            "      ssim:[\n",
            "        type: calculate_ssim\n",
            "        crop_border: 4\n",
            "        test_y_channel: True\n",
            "        better: higher\n",
            "      ]\n",
            "    ]\n",
            "  ]\n",
            "  logger:[\n",
            "    print_freq: 50\n",
            "    save_checkpoint_freq: 1000.0\n",
            "    use_tb_logger: True\n",
            "    wandb:[\n",
            "      project: None\n",
            "      resume_id: None\n",
            "    ]\n",
            "  ]\n",
            "  dist_params:[\n",
            "    backend: nccl\n",
            "    port: 29500\n",
            "  ]\n",
            "  dist: False\n",
            "  rank: 0\n",
            "  world_size: 1\n",
            "  auto_resume: False\n",
            "  is_train: True\n",
            "  root_path: /content/HAT\n",
            "\n",
            "2023-10-12 18:35:41.033182: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-12 18:35:42.139611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-10-12 18:35:43,705 INFO: Dataset [PairedImageDataset] - DIV2K is built.\n",
            "2023-10-12 18:35:43,705 INFO: Training statistics:\n",
            "\tNumber of train images: 800\n",
            "\tDataset enlarge ratio: 1\n",
            "\tBatch size per gpu: 4\n",
            "\tWorld size (gpu number): 1\n",
            "\tRequire iter number per epoch: 200\n",
            "\tTotal epochs: 1250; iters: 250000.\n",
            "2023-10-12 18:35:43,706 INFO: Dataset [PairedImageDataset] - Set5 is built.\n",
            "2023-10-12 18:35:43,706 INFO: Number of val images/folders in Set5: 5\n",
            "2023-10-12 18:35:43,707 INFO: Dataset [PairedImageDataset] - Set14 is built.\n",
            "2023-10-12 18:35:43,707 INFO: Number of val images/folders in Set14: 14\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "2023-10-12 18:35:43,919 INFO: Network [HAT] is created.\n",
            "2023-10-12 18:35:44,212 INFO: Network: HAT, with parameters: 9,621,183\n",
            "2023-10-12 18:35:44,212 INFO: HAT(\n",
            "  (conv_first): Conv2d(3, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (patch_unembed): PatchUnEmbed()\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (layers): ModuleList(\n",
            "    (0): RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0): HAB(\n",
            "            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(144, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(6, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1-5): 5 x HAB(\n",
            "            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(144, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(6, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (1-5): 5 x RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0-5): 6 x HAB(\n",
            "            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(144, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(6, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "  (conv_after_body): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_before_upsample): Sequential(\n",
            "    (0): Conv2d(144, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "  )\n",
            "  (upsample): Upsample(\n",
            "    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): PixelShuffle(upscale_factor=2)\n",
            "    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): PixelShuffle(upscale_factor=2)\n",
            "  )\n",
            "  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "2023-10-12 18:35:44,216 INFO: Use Exponential Moving Average with decay: 0.999\n",
            "2023-10-12 18:35:44,461 INFO: Network [HAT] is created.\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:04<00:00, 143MB/s]\n",
            "2023-10-12 18:35:50,825 INFO: Loss [PerceptualLoss] is created.\n",
            "2023-10-12 18:35:50,856 INFO: Model [HATModel] is created.\n",
            "2023-10-12 18:35:50,899 INFO: Start training from epoch: 0, iter: 0\n",
            "2023-10-12 18:37:04,242 INFO: [train..][epoch:  0, iter:      50, lr:(1.000e-04,)] [eta: 3 days, 15:40:58, time (data): 1.467 (0.035)] l_percep: 2.2325e+00 \n",
            "2023-10-12 18:38:12,109 INFO: [train..][epoch:  0, iter:     100, lr:(1.000e-04,)] [eta: 3 days, 18:54:42, time (data): 1.412 (0.019)] l_percep: 1.9591e+00 \n",
            "2023-10-12 18:39:21,947 INFO: [train..][epoch:  0, iter:     150, lr:(1.000e-04,)] [eta: 3 days, 20:53:42, time (data): 1.407 (0.014)] l_percep: 2.4669e+00 \n",
            "2023-10-12 18:40:32,306 INFO: [train..][epoch:  0, iter:     200, lr:(1.000e-04,)] [eta: 3 days, 22:03:44, time (data): 1.407 (0.011)] l_percep: 1.3610e+00 \n",
            "2023-10-12 18:41:44,462 INFO: [train..][epoch:  1, iter:     250, lr:(1.000e-04,)] [eta: 3 days, 23:15:10, time (data): 1.417 (0.005)] l_percep: 1.5209e+00 \n",
            "2023-10-12 18:42:54,963 INFO: [train..][epoch:  1, iter:     300, lr:(1.000e-04,)] [eta: 3 days, 23:39:36, time (data): 1.413 (0.005)] l_percep: 2.2557e+00 \n",
            "2023-10-12 18:44:05,610 INFO: [train..][epoch:  1, iter:     350, lr:(1.000e-04,)] [eta: 3 days, 23:58:29, time (data): 1.413 (0.005)] l_percep: 2.4745e+00 \n",
            "2023-10-12 18:45:15,962 INFO: [train..][epoch:  1, iter:     400, lr:(1.000e-04,)] [eta: 4 days, 0:09:17, time (data): 1.412 (0.005)] l_percep: 2.0676e+00 \n",
            "2023-10-12 18:46:27,516 INFO: [train..][epoch:  2, iter:     450, lr:(1.000e-04,)] [eta: 4 days, 0:28:31, time (data): 1.409 (0.005)] l_percep: 1.7096e+00 \n",
            "2023-10-12 18:47:38,054 INFO: [train..][epoch:  2, iter:     500, lr:(1.000e-04,)] [eta: 4 days, 0:35:15, time (data): 1.410 (0.006)] l_percep: 1.7716e+00 \n",
            "2023-10-12 18:48:48,730 INFO: [train..][epoch:  2, iter:     550, lr:(1.000e-04,)] [eta: 4 days, 0:41:35, time (data): 1.411 (0.006)] l_percep: 2.0507e+00 \n",
            "2023-10-12 18:49:59,163 INFO: [train..][epoch:  2, iter:     600, lr:(1.000e-04,)] [eta: 4 days, 0:44:59, time (data): 1.411 (0.005)] l_percep: 1.7283e+00 \n",
            "2023-10-12 18:51:10,533 INFO: [train..][epoch:  3, iter:     650, lr:(1.000e-04,)] [eta: 4 days, 0:53:40, time (data): 1.411 (0.006)] l_percep: 1.6368e+00 \n",
            "2023-10-12 18:52:20,981 INFO: [train..][epoch:  3, iter:     700, lr:(1.000e-04,)] [eta: 4 days, 0:55:28, time (data): 1.410 (0.005)] l_percep: 2.5584e+00 \n",
            "2023-10-12 18:53:31,589 INFO: [train..][epoch:  3, iter:     750, lr:(1.000e-04,)] [eta: 4 days, 0:57:46, time (data): 1.411 (0.005)] l_percep: 2.3897e+00 \n",
            "2023-10-12 18:54:41,962 INFO: [train..][epoch:  3, iter:     800, lr:(1.000e-04,)] [eta: 4 days, 0:58:25, time (data): 1.410 (0.005)] l_percep: 1.4642e+00 \n",
            "2023-10-12 18:55:53,527 INFO: [train..][epoch:  4, iter:     850, lr:(1.000e-04,)] [eta: 4 days, 1:04:40, time (data): 1.417 (0.006)] l_percep: 2.0080e+00 \n",
            "2023-10-12 18:57:04,197 INFO: [train..][epoch:  4, iter:     900, lr:(1.000e-04,)] [eta: 4 days, 1:05:57, time (data): 1.415 (0.006)] l_percep: 1.9034e+00 \n",
            "2023-10-12 18:58:14,922 INFO: [train..][epoch:  4, iter:     950, lr:(1.000e-04,)] [eta: 4 days, 1:07:14, time (data): 1.415 (0.006)] l_percep: 1.7891e+00 \n",
            "2023-10-12 18:59:25,521 INFO: [train..][epoch:  4, iter:   1,000, lr:(1.000e-04,)] [eta: 4 days, 1:07:45, time (data): 1.414 (0.006)] l_percep: 1.4352e+00 \n",
            "2023-10-12 18:59:25,521 INFO: Saving models and training states.\n",
            "2023-10-12 19:00:37,581 INFO: [train..][epoch:  5, iter:   1,050, lr:(1.000e-04,)] [eta: 4 days, 1:13:51, time (data): 1.412 (0.005)] l_percep: 1.9551e+00 \n",
            "2023-10-12 19:01:48,305 INFO: [train..][epoch:  5, iter:   1,100, lr:(1.000e-04,)] [eta: 4 days, 1:14:17, time (data): 1.413 (0.005)] l_percep: 1.7937e+00 \n",
            "2023-10-12 19:02:58,825 INFO: [train..][epoch:  5, iter:   1,150, lr:(1.000e-04,)] [eta: 4 days, 1:13:49, time (data): 1.412 (0.005)] l_percep: 1.5829e+00 \n",
            "2023-10-12 19:04:09,433 INFO: [train..][epoch:  5, iter:   1,200, lr:(1.000e-04,)] [eta: 4 days, 1:13:37, time (data): 1.412 (0.005)] l_percep: 2.3380e+00 \n",
            "2023-10-12 19:05:20,988 INFO: [train..][epoch:  6, iter:   1,250, lr:(1.000e-04,)] [eta: 4 days, 1:16:28, time (data): 1.413 (0.005)] l_percep: 1.0016e+00 \n",
            "2023-10-12 19:06:31,477 INFO: [train..][epoch:  6, iter:   1,300, lr:(1.000e-04,)] [eta: 4 days, 1:15:36, time (data): 1.411 (0.005)] l_percep: 2.7272e+00 \n",
            "2023-10-12 19:07:41,921 INFO: [train..][epoch:  6, iter:   1,350, lr:(1.000e-04,)] [eta: 4 days, 1:14:35, time (data): 1.410 (0.005)] l_percep: 1.4626e+00 \n",
            "2023-10-12 19:08:52,579 INFO: [train..][epoch:  6, iter:   1,400, lr:(1.000e-04,)] [eta: 4 days, 1:14:11, time (data): 1.411 (0.005)] l_percep: 2.2284e+00 \n",
            "2023-10-12 19:10:04,132 INFO: [train..][epoch:  7, iter:   1,450, lr:(1.000e-04,)] [eta: 4 days, 1:16:18, time (data): 1.411 (0.005)] l_percep: 2.3130e+00 \n",
            "2023-10-12 19:11:14,726 INFO: [train..][epoch:  7, iter:   1,500, lr:(1.000e-04,)] [eta: 4 days, 1:15:32, time (data): 1.412 (0.005)] l_percep: 1.7483e+00 \n",
            "2023-10-12 19:12:25,306 INFO: [train..][epoch:  7, iter:   1,550, lr:(1.000e-04,)] [eta: 4 days, 1:14:42, time (data): 1.412 (0.005)] l_percep: 1.4615e+00 \n",
            "2023-10-12 19:13:35,932 INFO: [train..][epoch:  7, iter:   1,600, lr:(1.000e-04,)] [eta: 4 days, 1:13:59, time (data): 1.412 (0.005)] l_percep: 1.6736e+00 \n",
            "2023-10-12 19:14:47,551 INFO: [train..][epoch:  8, iter:   1,650, lr:(1.000e-04,)] [eta: 4 days, 1:15:43, time (data): 1.415 (0.006)] l_percep: 2.7169e+00 \n",
            "2023-10-12 19:15:58,563 INFO: [train..][epoch:  8, iter:   1,700, lr:(1.000e-04,)] [eta: 4 days, 1:15:48, time (data): 1.418 (0.006)] l_percep: 2.1772e+00 \n",
            "2023-10-12 19:17:09,100 INFO: [train..][epoch:  8, iter:   1,750, lr:(1.000e-04,)] [eta: 4 days, 1:14:41, time (data): 1.415 (0.005)] l_percep: 1.9044e+00 \n",
            "2023-10-12 19:18:19,552 INFO: [train..][epoch:  8, iter:   1,800, lr:(1.000e-04,)] [eta: 4 days, 1:13:23, time (data): 1.414 (0.005)] l_percep: 1.4814e+00 \n",
            "2023-10-12 19:19:31,091 INFO: [train..][epoch:  9, iter:   1,850, lr:(1.000e-04,)] [eta: 4 days, 1:14:30, time (data): 1.414 (0.005)] l_percep: 1.8822e+00 \n",
            "2023-10-12 19:20:41,662 INFO: [train..][epoch:  9, iter:   1,900, lr:(1.000e-04,)] [eta: 4 days, 1:13:24, time (data): 1.413 (0.005)] l_percep: 1.5630e+00 \n",
            "2023-10-12 19:21:52,468 INFO: [train..][epoch:  9, iter:   1,950, lr:(1.000e-04,)] [eta: 4 days, 1:12:48, time (data): 1.414 (0.005)] l_percep: 1.6718e+00 \n",
            "2023-10-12 19:23:03,032 INFO: [train..][epoch:  9, iter:   2,000, lr:(1.000e-04,)] [eta: 4 days, 1:11:40, time (data): 1.413 (0.005)] l_percep: 1.9038e+00 \n",
            "2023-10-12 19:23:03,033 INFO: Saving models and training states.\n",
            "2023-10-12 19:24:15,067 INFO: [train..][epoch: 10, iter:   2,050, lr:(1.000e-04,)] [eta: 4 days, 1:13:29, time (data): 1.412 (0.006)] l_percep: 1.6644e+00 \n",
            "2023-10-12 19:25:25,669 INFO: [train..][epoch: 10, iter:   2,100, lr:(1.000e-04,)] [eta: 4 days, 1:12:21, time (data): 1.412 (0.005)] l_percep: 1.1906e+00 \n",
            "2023-10-12 19:26:36,262 INFO: [train..][epoch: 10, iter:   2,150, lr:(1.000e-04,)] [eta: 4 days, 1:11:12, time (data): 1.412 (0.005)] l_percep: 1.9671e+00 \n",
            "2023-10-12 19:27:47,066 INFO: [train..][epoch: 10, iter:   2,200, lr:(1.000e-04,)] [eta: 4 days, 1:10:26, time (data): 1.413 (0.005)] l_percep: 1.9038e+00 \n",
            "2023-10-12 19:28:58,780 INFO: [train..][epoch: 11, iter:   2,250, lr:(1.000e-04,)] [eta: 4 days, 1:11:20, time (data): 1.414 (0.005)] l_percep: 1.4871e+00 \n",
            "2023-10-12 19:30:09,478 INFO: [train..][epoch: 11, iter:   2,300, lr:(1.000e-04,)] [eta: 4 days, 1:10:19, time (data): 1.414 (0.005)] l_percep: 2.1144e+00 \n",
            "2023-10-12 19:31:20,110 INFO: [train..][epoch: 11, iter:   2,350, lr:(1.000e-04,)] [eta: 4 days, 1:09:10, time (data): 1.413 (0.005)] l_percep: 1.4038e+00 \n",
            "2023-10-12 19:32:30,806 INFO: [train..][epoch: 11, iter:   2,400, lr:(1.000e-04,)] [eta: 4 days, 1:08:08, time (data): 1.413 (0.005)] l_percep: 1.5853e+00 \n",
            "2023-10-12 19:33:42,922 INFO: [train..][epoch: 12, iter:   2,450, lr:(1.000e-04,)] [eta: 4 days, 1:09:29, time (data): 1.412 (0.005)] l_percep: 1.2930e+00 \n",
            "2023-10-12 19:34:53,516 INFO: [train..][epoch: 12, iter:   2,500, lr:(1.000e-04,)] [eta: 4 days, 1:08:13, time (data): 1.412 (0.005)] l_percep: 1.9397e+00 \n",
            "2023-10-12 19:36:04,028 INFO: [train..][epoch: 12, iter:   2,550, lr:(1.000e-04,)] [eta: 4 days, 1:06:49, time (data): 1.411 (0.005)] l_percep: 1.5068e+00 \n",
            "2023-10-12 19:37:14,567 INFO: [train..][epoch: 12, iter:   2,600, lr:(1.000e-04,)] [eta: 4 days, 1:05:29, time (data): 1.411 (0.005)] l_percep: 2.0206e+00 \n",
            "2023-10-12 19:38:26,826 INFO: [train..][epoch: 13, iter:   2,650, lr:(1.000e-04,)] [eta: 4 days, 1:06:49, time (data): 1.413 (0.006)] l_percep: 1.3928e+00 \n",
            "2023-10-12 19:39:37,311 INFO: [train..][epoch: 13, iter:   2,700, lr:(1.000e-04,)] [eta: 4 days, 1:05:21, time (data): 1.411 (0.005)] l_percep: 1.8092e+00 \n",
            "2023-10-12 19:40:47,962 INFO: [train..][epoch: 13, iter:   2,750, lr:(1.000e-04,)] [eta: 4 days, 1:04:09, time (data): 1.412 (0.006)] l_percep: 1.7291e+00 \n",
            "2023-10-12 19:41:58,598 INFO: [train..][epoch: 13, iter:   2,800, lr:(1.000e-04,)] [eta: 4 days, 1:02:56, time (data): 1.412 (0.006)] l_percep: 1.4514e+00 \n",
            "2023-10-12 19:43:10,279 INFO: [train..][epoch: 14, iter:   2,850, lr:(1.000e-04,)] [eta: 4 days, 1:03:13, time (data): 1.409 (0.005)] l_percep: 1.3319e+00 \n",
            "2023-10-12 19:44:20,958 INFO: [train..][epoch: 14, iter:   2,900, lr:(1.000e-04,)] [eta: 4 days, 1:02:02, time (data): 1.412 (0.005)] l_percep: 1.5276e+00 \n",
            "2023-10-12 19:45:31,795 INFO: [train..][epoch: 14, iter:   2,950, lr:(1.000e-04,)] [eta: 4 days, 1:01:04, time (data): 1.414 (0.005)] l_percep: 1.5413e+00 \n",
            "2023-10-12 19:46:42,471 INFO: [train..][epoch: 14, iter:   3,000, lr:(1.000e-04,)] [eta: 4 days, 0:59:52, time (data): 1.414 (0.005)] l_percep: 2.0045e+00 \n",
            "2023-10-12 19:46:42,472 INFO: Saving models and training states.\n",
            "2023-10-12 19:47:54,512 INFO: [train..][epoch: 15, iter:   3,050, lr:(1.000e-04,)] [eta: 4 days, 1:00:31, time (data): 1.414 (0.005)] l_percep: 1.3984e+00 \n",
            "2023-10-12 19:49:05,108 INFO: [train..][epoch: 15, iter:   3,100, lr:(1.000e-04,)] [eta: 4 days, 0:59:11, time (data): 1.413 (0.005)] l_percep: 1.7501e+00 \n",
            "2023-10-12 19:50:15,725 INFO: [train..][epoch: 15, iter:   3,150, lr:(1.000e-04,)] [eta: 4 days, 0:57:54, time (data): 1.413 (0.005)] l_percep: 1.3840e+00 \n",
            "2023-10-12 19:51:26,215 INFO: [train..][epoch: 15, iter:   3,200, lr:(1.000e-04,)] [eta: 4 days, 0:56:26, time (data): 1.412 (0.005)] l_percep: 1.4611e+00 \n",
            "2023-10-12 19:52:37,949 INFO: [train..][epoch: 16, iter:   3,250, lr:(1.000e-04,)] [eta: 4 days, 0:56:34, time (data): 1.415 (0.005)] l_percep: 1.0731e+00 \n",
            "2023-10-12 19:53:48,632 INFO: [train..][epoch: 16, iter:   3,300, lr:(1.000e-04,)] [eta: 4 days, 0:55:20, time (data): 1.414 (0.005)] l_percep: 2.5244e+00 \n",
            "2023-10-12 19:54:59,325 INFO: [train..][epoch: 16, iter:   3,350, lr:(1.000e-04,)] [eta: 4 days, 0:54:08, time (data): 1.414 (0.005)] l_percep: 1.6786e+00 \n",
            "2023-10-12 19:56:09,961 INFO: [train..][epoch: 16, iter:   3,400, lr:(1.000e-04,)] [eta: 4 days, 0:52:51, time (data): 1.414 (0.005)] l_percep: 1.9517e+00 \n",
            "2023-10-12 19:57:21,659 INFO: [train..][epoch: 17, iter:   3,450, lr:(1.000e-04,)] [eta: 4 days, 0:52:51, time (data): 1.419 (0.006)] l_percep: 1.7976e+00 \n",
            "2023-10-12 19:58:32,341 INFO: [train..][epoch: 17, iter:   3,500, lr:(1.000e-04,)] [eta: 4 days, 0:51:36, time (data): 1.416 (0.005)] l_percep: 2.6104e+00 \n",
            "2023-10-12 19:59:42,949 INFO: [train..][epoch: 17, iter:   3,550, lr:(1.000e-04,)] [eta: 4 days, 0:50:17, time (data): 1.414 (0.005)] l_percep: 1.8507e+00 \n",
            "2023-10-12 20:00:53,632 INFO: [train..][epoch: 17, iter:   3,600, lr:(1.000e-04,)] [eta: 4 days, 0:49:03, time (data): 1.414 (0.005)] l_percep: 2.0106e+00 \n",
            "2023-10-12 20:02:05,281 INFO: [train..][epoch: 18, iter:   3,650, lr:(1.000e-04,)] [eta: 4 days, 0:48:55, time (data): 1.417 (0.006)] l_percep: 1.9240e+00 \n",
            "2023-10-12 20:03:15,805 INFO: [train..][epoch: 18, iter:   3,700, lr:(1.000e-04,)] [eta: 4 days, 0:47:30, time (data): 1.413 (0.005)] l_percep: 1.9250e+00 \n",
            "2023-10-12 20:04:26,339 INFO: [train..][epoch: 18, iter:   3,750, lr:(1.000e-04,)] [eta: 4 days, 0:46:06, time (data): 1.412 (0.006)] l_percep: 1.8889e+00 \n",
            "2023-10-12 20:05:36,874 INFO: [train..][epoch: 18, iter:   3,800, lr:(1.000e-04,)] [eta: 4 days, 0:44:42, time (data): 1.412 (0.006)] l_percep: 1.6607e+00 \n",
            "2023-10-12 20:06:48,469 INFO: [train..][epoch: 19, iter:   3,850, lr:(1.000e-04,)] [eta: 4 days, 0:44:26, time (data): 1.414 (0.005)] l_percep: 2.0520e+00 \n",
            "2023-10-12 20:07:59,211 INFO: [train..][epoch: 19, iter:   3,900, lr:(1.000e-04,)] [eta: 4 days, 0:43:16, time (data): 1.414 (0.006)] l_percep: 1.2785e+00 \n",
            "2023-10-12 20:09:09,833 INFO: [train..][epoch: 19, iter:   3,950, lr:(1.000e-04,)] [eta: 4 days, 0:41:57, time (data): 1.414 (0.006)] l_percep: 1.7169e+00 \n",
            "2023-10-12 20:10:20,841 INFO: [train..][epoch: 19, iter:   4,000, lr:(1.000e-04,)] [eta: 4 days, 0:41:03, time (data): 1.415 (0.006)] l_percep: 1.8614e+00 \n",
            "2023-10-12 20:10:20,842 INFO: Saving models and training states.\n",
            "2023-10-12 20:11:33,121 INFO: [train..][epoch: 20, iter:   4,050, lr:(1.000e-04,)] [eta: 4 days, 0:41:25, time (data): 1.414 (0.005)] l_percep: 1.8650e+00 \n",
            "2023-10-12 20:12:43,771 INFO: [train..][epoch: 20, iter:   4,100, lr:(1.000e-04,)] [eta: 4 days, 0:40:08, time (data): 1.413 (0.005)] l_percep: 1.5896e+00 \n",
            "2023-10-12 20:13:54,299 INFO: [train..][epoch: 20, iter:   4,150, lr:(1.000e-04,)] [eta: 4 days, 0:38:43, time (data): 1.412 (0.005)] l_percep: 2.5859e+00 \n",
            "2023-10-12 20:15:04,825 INFO: [train..][epoch: 20, iter:   4,200, lr:(1.000e-04,)] [eta: 4 days, 0:37:19, time (data): 1.412 (0.005)] l_percep: 1.4641e+00 \n",
            "2023-10-12 20:16:17,279 INFO: [train..][epoch: 21, iter:   4,250, lr:(1.000e-04,)] [eta: 4 days, 0:37:46, time (data): 1.414 (0.005)] l_percep: 2.7703e+00 \n",
            "2023-10-12 20:17:27,946 INFO: [train..][epoch: 21, iter:   4,300, lr:(1.000e-04,)] [eta: 4 days, 0:36:29, time (data): 1.414 (0.005)] l_percep: 1.6479e+00 \n",
            "2023-10-12 20:18:38,416 INFO: [train..][epoch: 21, iter:   4,350, lr:(1.000e-04,)] [eta: 4 days, 0:35:01, time (data): 1.412 (0.005)] l_percep: 2.7063e+00 \n",
            "2023-10-12 20:19:48,995 INFO: [train..][epoch: 21, iter:   4,400, lr:(1.000e-04,)] [eta: 4 days, 0:33:40, time (data): 1.412 (0.005)] l_percep: 1.4426e+00 \n",
            "2023-10-12 20:21:00,749 INFO: [train..][epoch: 22, iter:   4,450, lr:(1.000e-04,)] [eta: 4 days, 0:33:23, time (data): 1.419 (0.006)] l_percep: 1.6294e+00 \n",
            "2023-10-12 20:22:11,699 INFO: [train..][epoch: 22, iter:   4,500, lr:(1.000e-04,)] [eta: 4 days, 0:32:21, time (data): 1.419 (0.006)] l_percep: 1.8570e+00 \n",
            "2023-10-12 20:23:22,429 INFO: [train..][epoch: 22, iter:   4,550, lr:(1.000e-04,)] [eta: 4 days, 0:31:08, time (data): 1.417 (0.006)] l_percep: 1.9570e+00 \n",
            "2023-10-12 20:24:32,712 INFO: [train..][epoch: 22, iter:   4,600, lr:(1.000e-04,)] [eta: 4 days, 0:29:30, time (data): 1.414 (0.005)] l_percep: 1.2665e+00 \n",
            "2023-10-12 20:25:44,839 INFO: [train..][epoch: 23, iter:   4,650, lr:(1.000e-04,)] [eta: 4 days, 0:29:31, time (data): 1.410 (0.005)] l_percep: 2.0534e+00 \n",
            "2023-10-12 20:26:55,565 INFO: [train..][epoch: 23, iter:   4,700, lr:(1.000e-04,)] [eta: 4 days, 0:28:17, time (data): 1.413 (0.005)] l_percep: 1.8003e+00 \n",
            "2023-10-12 20:28:06,296 INFO: [train..][epoch: 23, iter:   4,750, lr:(1.000e-04,)] [eta: 4 days, 0:27:03, time (data): 1.414 (0.006)] l_percep: 1.8102e+00 \n",
            "2023-10-12 20:29:16,860 INFO: [train..][epoch: 23, iter:   4,800, lr:(1.000e-04,)] [eta: 4 days, 0:25:41, time (data): 1.413 (0.005)] l_percep: 1.3772e+00 \n",
            "2023-10-12 20:30:29,148 INFO: [train..][epoch: 24, iter:   4,850, lr:(1.000e-04,)] [eta: 4 days, 0:25:46, time (data): 1.411 (0.005)] l_percep: 1.3628e+00 \n",
            "2023-10-12 20:31:39,879 INFO: [train..][epoch: 24, iter:   4,900, lr:(1.000e-04,)] [eta: 4 days, 0:24:31, time (data): 1.413 (0.005)] l_percep: 1.4468e+00 \n",
            "2023-10-12 20:32:50,431 INFO: [train..][epoch: 24, iter:   4,950, lr:(1.000e-04,)] [eta: 4 days, 0:23:08, time (data): 1.412 (0.005)] l_percep: 1.9162e+00 \n",
            "2023-10-12 20:34:00,902 INFO: [train..][epoch: 24, iter:   5,000, lr:(1.000e-04,)] [eta: 4 days, 0:21:41, time (data): 1.412 (0.005)] l_percep: 2.0773e+00 \n",
            "2023-10-12 20:34:00,902 INFO: Saving models and training states.\n",
            "2023-10-12 20:34:02,242 WARNING: Multiple validation datasets are *only* supported by SRModel.\n",
            "2023-10-12 20:34:05,425 INFO: Validation Set5\n",
            "\t # psnr: 10.5064\tBest: 10.5064 @ 5000 iter\n",
            "\t # ssim: 0.0236\tBest: 0.0236 @ 5000 iter\n",
            "\n",
            "2023-10-12 20:34:18,026 INFO: Validation Set14\n",
            "\t # psnr: 11.2431\tBest: 11.2431 @ 5000 iter\n",
            "\t # ssim: 0.0283\tBest: 0.0283 @ 5000 iter\n",
            "\n",
            "2023-10-12 20:35:29,337 INFO: [train..][epoch: 25, iter:   5,050, lr:(1.000e-04,)] [eta: 4 days, 0:34:46, time (data): 1.409 (0.004)] l_percep: 2.1290e+00 \n",
            "2023-10-12 20:36:39,909 INFO: [train..][epoch: 25, iter:   5,100, lr:(1.000e-04,)] [eta: 4 days, 0:33:16, time (data): 1.411 (0.005)] l_percep: 1.6571e+00 \n",
            "2023-10-12 20:37:50,513 INFO: [train..][epoch: 25, iter:   5,150, lr:(1.000e-04,)] [eta: 4 days, 0:31:47, time (data): 1.411 (0.005)] l_percep: 1.8297e+00 \n",
            "2023-10-12 20:39:01,082 INFO: [train..][epoch: 25, iter:   5,200, lr:(1.000e-04,)] [eta: 4 days, 0:30:18, time (data): 1.411 (0.005)] l_percep: 1.6097e+00 \n",
            "2023-10-12 20:40:12,579 INFO: [train..][epoch: 26, iter:   5,250, lr:(1.000e-04,)] [eta: 4 days, 0:29:32, time (data): 1.415 (0.005)] l_percep: 1.5223e+00 \n",
            "2023-10-12 20:41:23,202 INFO: [train..][epoch: 26, iter:   5,300, lr:(1.000e-04,)] [eta: 4 days, 0:28:05, time (data): 1.413 (0.005)] l_percep: 2.8162e+00 \n",
            "2023-10-12 20:42:33,876 INFO: [train..][epoch: 26, iter:   5,350, lr:(1.000e-04,)] [eta: 4 days, 0:26:41, time (data): 1.413 (0.005)] l_percep: 1.7595e+00 \n",
            "2023-10-12 20:43:44,639 INFO: [train..][epoch: 26, iter:   5,400, lr:(1.000e-04,)] [eta: 4 days, 0:25:21, time (data): 1.414 (0.005)] l_percep: 2.1524e+00 \n",
            "2023-10-12 20:44:56,356 INFO: [train..][epoch: 27, iter:   5,450, lr:(1.000e-04,)] [eta: 4 days, 0:24:44, time (data): 1.425 (0.006)] l_percep: 1.2343e+00 \n",
            "2023-10-12 20:46:06,932 INFO: [train..][epoch: 27, iter:   5,500, lr:(1.000e-04,)] [eta: 4 days, 0:23:16, time (data): 1.416 (0.006)] l_percep: 2.2545e+00 \n",
            "2023-10-12 20:47:17,730 INFO: [train..][epoch: 27, iter:   5,550, lr:(1.000e-04,)] [eta: 4 days, 0:21:58, time (data): 1.416 (0.005)] l_percep: 2.3979e+00 \n",
            "2023-10-12 20:48:28,542 INFO: [train..][epoch: 27, iter:   5,600, lr:(1.000e-04,)] [eta: 4 days, 0:20:40, time (data): 1.416 (0.005)] l_percep: 2.2695e+00 \n",
            "2023-10-12 20:49:40,264 INFO: [train..][epoch: 28, iter:   5,650, lr:(1.000e-04,)] [eta: 4 days, 0:20:03, time (data): 1.416 (0.005)] l_percep: 1.2846e+00 \n",
            "2023-10-12 20:50:50,852 INFO: [train..][epoch: 28, iter:   5,700, lr:(1.000e-04,)] [eta: 4 days, 0:18:35, time (data): 1.413 (0.005)] l_percep: 1.3240e+00 \n",
            "2023-10-12 20:52:01,546 INFO: [train..][epoch: 28, iter:   5,750, lr:(1.000e-04,)] [eta: 4 days, 0:17:13, time (data): 1.413 (0.005)] l_percep: 1.5348e+00 \n",
            "2023-10-12 20:53:12,093 INFO: [train..][epoch: 28, iter:   5,800, lr:(1.000e-04,)] [eta: 4 days, 0:15:45, time (data): 1.413 (0.005)] l_percep: 1.5321e+00 \n",
            "2023-10-12 20:54:23,740 INFO: [train..][epoch: 29, iter:   5,850, lr:(1.000e-04,)] [eta: 4 days, 0:15:03, time (data): 1.408 (0.005)] l_percep: 2.0812e+00 \n",
            "2023-10-12 20:55:34,486 INFO: [train..][epoch: 29, iter:   5,900, lr:(1.000e-04,)] [eta: 4 days, 0:13:43, time (data): 1.413 (0.006)] l_percep: 2.1931e+00 \n",
            "2023-10-12 20:56:45,131 INFO: [train..][epoch: 29, iter:   5,950, lr:(1.000e-04,)] [eta: 4 days, 0:12:19, time (data): 1.413 (0.006)] l_percep: 1.6802e+00 \n",
            "2023-10-12 20:57:55,778 INFO: [train..][epoch: 29, iter:   6,000, lr:(1.000e-04,)] [eta: 4 days, 0:10:55, time (data): 1.413 (0.006)] l_percep: 2.0113e+00 \n",
            "2023-10-12 20:57:55,778 INFO: Saving models and training states.\n",
            "2023-10-12 20:59:08,277 INFO: [train..][epoch: 30, iter:   6,050, lr:(1.000e-04,)] [eta: 4 days, 0:10:47, time (data): 1.411 (0.003)] l_percep: 1.6410e+00 \n",
            "2023-10-12 21:00:18,678 INFO: [train..][epoch: 30, iter:   6,100, lr:(1.000e-04,)] [eta: 4 days, 0:09:13, time (data): 1.409 (0.005)] l_percep: 2.0399e+00 \n",
            "2023-10-12 21:01:29,321 INFO: [train..][epoch: 30, iter:   6,150, lr:(1.000e-04,)] [eta: 4 days, 0:07:50, time (data): 1.410 (0.005)] l_percep: 1.4318e+00 \n",
            "2023-10-12 21:02:40,293 INFO: [train..][epoch: 30, iter:   6,200, lr:(1.000e-04,)] [eta: 4 days, 0:06:39, time (data): 1.413 (0.005)] l_percep: 1.2564e+00 \n",
            "2023-10-12 21:03:51,723 INFO: [train..][epoch: 31, iter:   6,250, lr:(1.000e-04,)] [eta: 4 days, 0:05:46, time (data): 1.416 (0.005)] l_percep: 1.6337e+00 \n",
            "2023-10-12 21:05:02,397 INFO: [train..][epoch: 31, iter:   6,300, lr:(1.000e-04,)] [eta: 4 days, 0:04:24, time (data): 1.414 (0.005)] l_percep: 2.0061e+00 \n",
            "2023-10-12 21:06:13,218 INFO: [train..][epoch: 31, iter:   6,350, lr:(1.000e-04,)] [eta: 4 days, 0:03:08, time (data): 1.415 (0.005)] l_percep: 1.7443e+00 \n",
            "2023-10-12 21:07:24,060 INFO: [train..][epoch: 31, iter:   6,400, lr:(1.000e-04,)] [eta: 4 days, 0:01:52, time (data): 1.416 (0.005)] l_percep: 1.7628e+00 \n",
            "2023-10-12 21:08:35,780 INFO: [train..][epoch: 32, iter:   6,450, lr:(1.000e-04,)] [eta: 4 days, 0:01:10, time (data): 1.415 (0.006)] l_percep: 1.2582e+00 \n",
            "2023-10-12 21:09:46,487 INFO: [train..][epoch: 32, iter:   6,500, lr:(1.000e-04,)] [eta: 3 days, 23:59:50, time (data): 1.414 (0.006)] l_percep: 1.9696e+00 \n",
            "2023-10-12 21:10:57,109 INFO: [train..][epoch: 32, iter:   6,550, lr:(1.000e-04,)] [eta: 3 days, 23:58:26, time (data): 1.414 (0.006)] l_percep: 2.0220e+00 \n",
            "2023-10-12 21:12:07,840 INFO: [train..][epoch: 32, iter:   6,600, lr:(1.000e-04,)] [eta: 3 days, 23:57:06, time (data): 1.414 (0.006)] l_percep: 1.5687e+00 \n",
            "2023-10-12 21:13:19,190 INFO: [train..][epoch: 33, iter:   6,650, lr:(1.000e-04,)] [eta: 3 days, 23:56:10, time (data): 1.414 (0.005)] l_percep: 1.3497e+00 \n",
            "2023-10-12 21:14:29,976 INFO: [train..][epoch: 33, iter:   6,700, lr:(1.000e-04,)] [eta: 3 days, 23:54:52, time (data): 1.415 (0.005)] l_percep: 1.7593e+00 \n",
            "2023-10-12 21:15:40,961 INFO: [train..][epoch: 33, iter:   6,750, lr:(1.000e-04,)] [eta: 3 days, 23:53:42, time (data): 1.417 (0.005)] l_percep: 1.5598e+00 \n",
            "2023-10-12 21:16:51,573 INFO: [train..][epoch: 33, iter:   6,800, lr:(1.000e-04,)] [eta: 3 days, 23:52:19, time (data): 1.416 (0.005)] l_percep: 2.6853e+00 \n",
            "2023-10-12 21:18:03,341 INFO: [train..][epoch: 34, iter:   6,850, lr:(1.000e-04,)] [eta: 3 days, 23:51:37, time (data): 1.423 (0.006)] l_percep: 1.9325e+00 \n",
            "2023-10-12 21:19:13,965 INFO: [train..][epoch: 34, iter:   6,900, lr:(1.000e-04,)] [eta: 3 days, 23:50:14, time (data): 1.415 (0.006)] l_percep: 1.9310e+00 \n",
            "2023-10-12 21:20:24,522 INFO: [train..][epoch: 34, iter:   6,950, lr:(1.000e-04,)] [eta: 3 days, 23:48:49, time (data): 1.413 (0.005)] l_percep: 1.5636e+00 \n",
            "2023-10-12 21:21:35,229 INFO: [train..][epoch: 34, iter:   7,000, lr:(1.000e-04,)] [eta: 3 days, 23:47:29, time (data): 1.414 (0.006)] l_percep: 1.6803e+00 \n",
            "2023-10-12 21:21:35,230 INFO: Saving models and training states.\n",
            "2023-10-12 21:22:47,224 INFO: [train..][epoch: 35, iter:   7,050, lr:(1.000e-04,)] [eta: 3 days, 23:46:54, time (data): 1.417 (0.005)] l_percep: 1.2611e+00 \n",
            "2023-10-12 21:23:57,625 INFO: [train..][epoch: 35, iter:   7,100, lr:(1.000e-04,)] [eta: 3 days, 23:45:24, time (data): 1.410 (0.005)] l_percep: 1.3485e+00 \n",
            "2023-10-12 21:25:08,216 INFO: [train..][epoch: 35, iter:   7,150, lr:(1.000e-04,)] [eta: 3 days, 23:44:00, time (data): 1.411 (0.005)] l_percep: 1.8690e+00 \n",
            "2023-10-12 21:26:18,904 INFO: [train..][epoch: 35, iter:   7,200, lr:(1.000e-04,)] [eta: 3 days, 23:42:40, time (data): 1.412 (0.005)] l_percep: 1.6126e+00 \n",
            "2023-10-12 21:27:30,761 INFO: [train..][epoch: 36, iter:   7,250, lr:(1.000e-04,)] [eta: 3 days, 23:41:59, time (data): 1.419 (0.007)] l_percep: 1.7154e+00 \n",
            "2023-10-12 21:28:41,334 INFO: [train..][epoch: 36, iter:   7,300, lr:(1.000e-04,)] [eta: 3 days, 23:40:35, time (data): 1.413 (0.006)] l_percep: 2.3431e+00 \n",
            "2023-10-12 21:29:51,763 INFO: [train..][epoch: 36, iter:   7,350, lr:(1.000e-04,)] [eta: 3 days, 23:39:07, time (data): 1.411 (0.005)] l_percep: 2.1528e+00 \n",
            "2023-10-12 21:31:02,516 INFO: [train..][epoch: 36, iter:   7,400, lr:(1.000e-04,)] [eta: 3 days, 23:37:49, time (data): 1.412 (0.005)] l_percep: 1.8596e+00 \n",
            "2023-10-12 21:32:14,072 INFO: [train..][epoch: 37, iter:   7,450, lr:(1.000e-04,)] [eta: 3 days, 23:36:58, time (data): 1.420 (0.008)] l_percep: 1.6945e+00 \n",
            "2023-10-12 21:33:24,718 INFO: [train..][epoch: 37, iter:   7,500, lr:(1.000e-04,)] [eta: 3 days, 23:35:37, time (data): 1.414 (0.006)] l_percep: 1.4888e+00 \n",
            "2023-10-12 21:34:35,572 INFO: [train..][epoch: 37, iter:   7,550, lr:(1.000e-04,)] [eta: 3 days, 23:34:23, time (data): 1.416 (0.006)] l_percep: 1.4883e+00 \n",
            "2023-10-12 21:35:46,274 INFO: [train..][epoch: 37, iter:   7,600, lr:(1.000e-04,)] [eta: 3 days, 23:33:03, time (data): 1.415 (0.006)] l_percep: 1.9762e+00 \n",
            "2023-10-12 21:36:57,965 INFO: [train..][epoch: 38, iter:   7,650, lr:(1.000e-04,)] [eta: 3 days, 23:32:16, time (data): 1.422 (0.006)] l_percep: 1.7037e+00 \n",
            "2023-10-12 21:38:08,677 INFO: [train..][epoch: 38, iter:   7,700, lr:(1.000e-04,)] [eta: 3 days, 23:30:57, time (data): 1.416 (0.005)] l_percep: 1.1825e+00 \n",
            "2023-10-12 21:39:19,355 INFO: [train..][epoch: 38, iter:   7,750, lr:(1.000e-04,)] [eta: 3 days, 23:29:37, time (data): 1.415 (0.005)] l_percep: 1.7525e+00 \n",
            "2023-10-12 21:40:30,346 INFO: [train..][epoch: 38, iter:   7,800, lr:(1.000e-04,)] [eta: 3 days, 23:28:28, time (data): 1.416 (0.005)] l_percep: 1.7389e+00 \n",
            "2023-10-12 21:41:41,728 INFO: [train..][epoch: 39, iter:   7,850, lr:(1.000e-04,)] [eta: 3 days, 23:27:30, time (data): 1.415 (0.005)] l_percep: 1.8497e+00 \n",
            "2023-10-12 21:42:52,497 INFO: [train..][epoch: 39, iter:   7,900, lr:(1.000e-04,)] [eta: 3 days, 23:26:13, time (data): 1.415 (0.006)] l_percep: 1.6399e+00 \n",
            "2023-10-12 21:44:03,217 INFO: [train..][epoch: 39, iter:   7,950, lr:(1.000e-04,)] [eta: 3 days, 23:24:55, time (data): 1.415 (0.006)] l_percep: 1.9577e+00 \n",
            "2023-10-12 21:45:13,963 INFO: [train..][epoch: 39, iter:   8,000, lr:(1.000e-04,)] [eta: 3 days, 23:23:38, time (data): 1.415 (0.006)] l_percep: 2.2902e+00 \n",
            "2023-10-12 21:45:13,964 INFO: Saving models and training states.\n",
            "2023-10-12 21:46:25,960 INFO: [train..][epoch: 40, iter:   8,050, lr:(1.000e-04,)] [eta: 3 days, 23:22:58, time (data): 1.413 (0.006)] l_percep: 2.1273e+00 \n",
            "2023-10-12 21:47:36,809 INFO: [train..][epoch: 40, iter:   8,100, lr:(1.000e-04,)] [eta: 3 days, 23:21:44, time (data): 1.416 (0.006)] l_percep: 1.9901e+00 \n",
            "2023-10-12 21:48:47,713 INFO: [train..][epoch: 40, iter:   8,150, lr:(1.000e-04,)] [eta: 3 days, 23:20:31, time (data): 1.417 (0.005)] l_percep: 1.9891e+00 \n",
            "2023-10-12 21:49:58,291 INFO: [train..][epoch: 40, iter:   8,200, lr:(1.000e-04,)] [eta: 3 days, 23:19:09, time (data): 1.415 (0.005)] l_percep: 1.4684e+00 \n",
            "2023-10-12 21:51:10,014 INFO: [train..][epoch: 41, iter:   8,250, lr:(1.000e-04,)] [eta: 3 days, 23:18:20, time (data): 1.413 (0.006)] l_percep: 1.1392e+00 \n",
            "2023-10-12 21:52:20,647 INFO: [train..][epoch: 41, iter:   8,300, lr:(1.000e-04,)] [eta: 3 days, 23:17:00, time (data): 1.413 (0.005)] l_percep: 1.6433e+00 \n",
            "2023-10-12 21:53:31,298 INFO: [train..][epoch: 41, iter:   8,350, lr:(1.000e-04,)] [eta: 3 days, 23:15:40, time (data): 1.413 (0.005)] l_percep: 2.1411e+00 \n",
            "2023-10-12 21:54:42,132 INFO: [train..][epoch: 41, iter:   8,400, lr:(1.000e-04,)] [eta: 3 days, 23:14:25, time (data): 1.414 (0.006)] l_percep: 1.7738e+00 \n",
            "2023-10-12 21:55:53,767 INFO: [train..][epoch: 42, iter:   8,450, lr:(1.000e-04,)] [eta: 3 days, 23:13:34, time (data): 1.413 (0.006)] l_percep: 1.9533e+00 \n",
            "2023-10-12 21:57:04,821 INFO: [train..][epoch: 42, iter:   8,500, lr:(1.000e-04,)] [eta: 3 days, 23:12:25, time (data): 1.420 (0.006)] l_percep: 2.5879e+00 \n",
            "2023-10-12 21:58:15,529 INFO: [train..][epoch: 42, iter:   8,550, lr:(1.000e-04,)] [eta: 3 days, 23:11:07, time (data): 1.417 (0.006)] l_percep: 1.7578e+00 \n",
            "2023-10-12 21:59:26,173 INFO: [train..][epoch: 42, iter:   8,600, lr:(1.000e-04,)] [eta: 3 days, 23:09:47, time (data): 1.416 (0.006)] l_percep: 2.3501e+00 \n",
            "2023-10-12 22:00:37,360 INFO: [train..][epoch: 43, iter:   8,650, lr:(1.000e-04,)] [eta: 3 days, 23:08:43, time (data): 1.416 (0.006)] l_percep: 1.9681e+00 \n",
            "2023-10-12 22:01:47,774 INFO: [train..][epoch: 43, iter:   8,700, lr:(1.000e-04,)] [eta: 3 days, 23:07:17, time (data): 1.409 (0.004)] l_percep: 1.6140e+00 \n",
            "2023-10-12 22:02:58,166 INFO: [train..][epoch: 43, iter:   8,750, lr:(1.000e-04,)] [eta: 3 days, 23:05:50, time (data): 1.409 (0.005)] l_percep: 1.4142e+00 \n",
            "2023-10-12 22:04:08,459 INFO: [train..][epoch: 43, iter:   8,800, lr:(1.000e-04,)] [eta: 3 days, 23:04:21, time (data): 1.408 (0.005)] l_percep: 1.9994e+00 \n",
            "2023-10-12 22:05:19,860 INFO: [train..][epoch: 44, iter:   8,850, lr:(1.000e-04,)] [eta: 3 days, 23:03:22, time (data): 1.408 (0.005)] l_percep: 1.5270e+00 \n",
            "2023-10-12 22:06:30,269 INFO: [train..][epoch: 44, iter:   8,900, lr:(1.000e-04,)] [eta: 3 days, 23:01:57, time (data): 1.408 (0.005)] l_percep: 2.2579e+00 \n",
            "2023-10-12 22:07:40,618 INFO: [train..][epoch: 44, iter:   8,950, lr:(1.000e-04,)] [eta: 3 days, 23:00:30, time (data): 1.408 (0.005)] l_percep: 1.6753e+00 \n",
            "2023-10-12 22:08:51,144 INFO: [train..][epoch: 44, iter:   9,000, lr:(1.000e-04,)] [eta: 3 days, 22:59:07, time (data): 1.408 (0.005)] l_percep: 2.3342e+00 \n",
            "2023-10-12 22:08:51,144 INFO: Saving models and training states.\n"
          ]
        }
      ],
      "source": [
        "!python hat/train.py -opt options/train/train_HAT-S_SRx4_PerceptualLoss.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save checkpoint"
      ],
      "metadata": {
        "id": "JZXksNBqMv8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cp -rf /content/HAT/experiments/train_HAT-S_SRx4_PerceptualLoss /content/drive/MyDrive/Results/superresolution"
      ],
      "metadata": {
        "id": "IdoCwxEzMFBm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}